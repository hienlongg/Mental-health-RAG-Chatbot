{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c49d7b",
   "metadata": {},
   "source": [
    "# 1. RAG-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101063da",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b13e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Verify required environment variables\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "if not langsmith_api_key:\n",
    "    raise EnvironmentError(\"LANGSMITH_API_KEY not set in environment. Add it to your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691ce39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a8839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538eff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3e725",
   "metadata": {},
   "source": [
    "## **1. Indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3911f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "if vector_store._collection.count() > 0:\n",
    "    pass\n",
    "else:\n",
    "    file_path = \"../data/documents/DSM-5 Các tiêu chuẩn chẩn đoán.pdf\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376e99f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdocs\u001b[49m[\u001b[32m0\u001b[39m].page_content[:\u001b[32m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[32m0\u001b[39m].metadata)\n",
      "\u001b[31mNameError\u001b[39m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"{docs[0].page_content[:200]}\\n\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 304 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cfaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 384\n",
      "\n",
      "[-0.04189952835440636, 0.0183077622205019, 0.014665888622403145, -0.04995323717594147, -0.047513555735349655, -0.030777601525187492, 0.034349385648965836, 0.05352432280778885, 0.07229814678430557, -0.0016910440754145384]\n"
     ]
    }
   ],
   "source": [
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
    "print(vector_1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7357da46-1f69-4521-8f5a-088d99ffb0dc', '88925797-ab79-4920-8011-62708352c536', '35d596f2-d163-41c8-98f1-9a1402e240d5']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db4aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store collection name: example_collection\n",
      "Number of documents in vector store: 2432\n",
      "\n",
      "Sample retrieved document:\n",
      "Content: nhân đang s ử dụng thuốc chủ vận đư ợc kê đơn như methadone hoặc\n",
      "buprenorphinekhông có tiêu chuẩn chẩn đoán cho r ối loạn sử dụng opioid cho\n",
      "loại thuốc này (trừ khả năng dung nạp hoặc trạng thái cai, \n",
      "Metadata: {'producer': 'Nitro Pro 7  (7. 5. 0. 29)', 'total_pages': 100, 'creationdate': '2016-12-23T13:13:10+00:00', 'page': 83, 'title': 'Chỉ sử dụng tài liệu vào mục đích học tập và nghiên cứu', 'page_label': '84', 'source': '../data/documents/DSM-5 Các tiêu chuẩn chẩn đoán.pdf', 'creator': 'Nitro Pro', 'author': 'Nguyen Sinh Phuc', 'moddate': '2016-12-23T20:16:03+07:00', 'start_index': 2389}\n"
     ]
    }
   ],
   "source": [
    "# Check vector store contents and statistics\n",
    "print(f\"Vector store collection name: {vector_store._collection.name}\")\n",
    "print(f\"Number of documents in vector store: {vector_store._collection.count()}\")\n",
    "\n",
    "# Retrieve a sample document\n",
    "sample_result = vector_store.similarity_search(\"DSM-5\", k=1)\n",
    "print(f\"\\nSample retrieved document:\")\n",
    "print(f\"Content: {sample_result[0].page_content[:200]}\")\n",
    "print(f\"Metadata: {sample_result[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593348f",
   "metadata": {},
   "source": [
    "## 2. **Retrieval and Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f526e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from pdf. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    "    \"You are an mental therapy\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbd44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = (\n",
    "#     \"Tôi cảm thấy mệt mỏi, không muốn làm việc gì cả.\"\n",
    "# )\n",
    "\n",
    "# for event in agent.stream(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "#     stream_mode=\"values\",\n",
    "# ):\n",
    "#     event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d17011",
   "metadata": {},
   "source": [
    "# 2. Chainlit App with RAG Integration\n",
    "\n",
    "The chatbot above (`on_message` handler) uses:\n",
    "- The **vector_store** initialized in Setup to retrieve relevant documents\n",
    "- The **model** for generating responses\n",
    "- The **retrieve_context** tool to search the knowledge base\n",
    "- The **update_diagnosis** tool to track user assessments\n",
    "\n",
    "**To run the Chainlit app:**\n",
    "```bash\n",
    "chainlit run <this_notebook> --headless\n",
    "```\n",
    "\n",
    "Or in a separate terminal/cell:\n",
    "```bash\n",
    "chainlit run app.py  # if you move the code to app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd995c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointer initialized. Make sure tables are created.\n",
      "--- Psychology Chatbot Agent Created Successfully ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid  # Used to generate unique thread IDs\n",
    "import chainlit as cl\n",
    "from typing import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Import all your LangChain components ---\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain.agents.middleware import (\n",
    "    SummarizationMiddleware,\n",
    "    dynamic_prompt,\n",
    "    ModelRequest,\n",
    ")\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import ToolMessage, BaseMessage\n",
    "\n",
    "# 1. DATABASE & CHECKPOINTER SETUP (Permanent Memory)\n",
    "\n",
    "# IMPORTANT: Set this in your environment *before* running chainlit\n",
    "# export POSTGRES_DB_URI=\"postgresql://user:pass@host:port/dbname\"\n",
    "# export GOOGLE_API_KEY=\"your_google_api_key\"\n",
    "\n",
    "DB_URI = os.environ.get(\"POSTGRES_DB_URI\")\n",
    "if not DB_URI:\n",
    "    raise ValueError(\n",
    "        \"POSTGRES_DB_URI environment variable not set. \"\n",
    "        \"Please set it to your PostgreSQL connection string.\"\n",
    "    )\n",
    "\n",
    "# Initialize the checkpointer for persistent state\n",
    "# This connection is created once and reused.\n",
    "checkpointer = InMemorySaver()\n",
    "try:\n",
    "    # checkpointer.setup() # Uncomment this line to run setup() once\n",
    "    # to create the necessary tables in your Postgres database.\n",
    "    # Run this manually or handle it in your deployment script.\n",
    "    print(\"Checkpointer initialized. Make sure tables are created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not run checkpointer.setup(). \"\n",
    "          f\"Ensure tables exist. Error: {e}\")\n",
    "\n",
    "# 2. DEFINE CUSTOM AGENT STATE (Your Bot's Memory Schema)\n",
    "\n",
    "class PsychologyAgentState(AgentState):\n",
    "    \"\"\"\n",
    "    Defines the short-term memory state for the chatbot.\n",
    "    'messages' is included by default.\n",
    "    \"\"\"\n",
    "    user_id: str  # Requirement #3\n",
    "    score: str = \"\"  # Requirement #7\n",
    "    content: str = \"\"  # Requirement #7\n",
    "    total_guess: str = \"\"  # Requirement #7\n",
    "\n",
    "# 3. DEFINE TOOLS\n",
    "\n",
    "@tool\n",
    "def retrieve_context(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant psychological context or information from the knowledge base.\n",
    "    Uses the vector store to find similar documents.\n",
    "    \"\"\"\n",
    "    print(f\"[Tool Call: retrieve_context] Query: {query}\")\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    \n",
    "    if retrieved_docs:\n",
    "        serialized = \"\\n\\n\".join(\n",
    "            (f\"Source: {doc.metadata}\\nContent: {doc.page_content[:500]}\")\n",
    "            for doc in retrieved_docs\n",
    "        )\n",
    "        return f\"Retrieved relevant information:\\n\\n{serialized}\"\n",
    "    else:\n",
    "        return f\"No relevant information found for: {query}\"\n",
    "\n",
    "@tool\n",
    "def update_diagnosis(\n",
    "    score: str = Field(description=\"Score of the user's mental health (e.g., anxiety level 1-10).\"),\n",
    "    content: str = Field(description=\"Summary/content of the user's mental health state.\"),\n",
    "    total_guess: str = Field(description=\"Total assessment/diagnosis of the user's mental health.\"),\n",
    "    runtime: ToolRuntime = None\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Update the agent's internal analysis of the user's mental health state.\n",
    "    Use this when you have gathered enough information to form an assessment.\n",
    "    \"\"\"\n",
    "    print(f\"[Tool Call: update_diagnosis] Score: {score}, Content: {content}\")\n",
    "    # Update the state directly by returning a Command\n",
    "    return Command(\n",
    "        update={\n",
    "            \"score\": score,\n",
    "            \"content\": content,\n",
    "            \"total_guess\": total_guess,\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=f\"Diagnosis updated. Score: {score}, Analysis: {content}\",\n",
    "                    tool_call_id=runtime.tool_call_id if runtime else \"manual\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 4. DEFINE MIDDLEWARE (Summarization & Dynamic Prompt)\n",
    "\n",
    "# --- Setup Models (Requirement #6) ---\n",
    "# Use a fast model for summarization\n",
    "summarizer_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\", \n",
    "    temperature=0\n",
    ")\n",
    "# Use your main model for the agent\n",
    "main_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# --- Middleware 1: Summarization (Requirement #2) ---\n",
    "summarizer_middleware = SummarizationMiddleware(\n",
    "    model=summarizer_model,\n",
    "    max_tokens_before_summary=4000, # Trigger summarization at 4k tokens\n",
    "    messages_to_keep=5, # Keep the 5 most recent messages\n",
    ")\n",
    "\n",
    "# --- Middleware 2: Dynamic Prompt (Requirement #4 & #5) ---\n",
    "@dynamic_prompt\n",
    "def dynamic_psychology_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"\n",
    "    Dynamically injects context and diagnosis into the system prompt.\n",
    "    \"\"\"\n",
    "    state = request.runtime.state\n",
    "    base_prompt = (\n",
    "        \"You are a psychology chatbot assistant. Your goal is to hold a \"\n",
    "        \"supportive conversation to consult, analyze, and (for reference only) \"\n",
    "        \"diagnose the user's state. Always be empathetic and supportive.\"\n",
    "    )\n",
    "    \n",
    "    if state.get(\"score\") and state.get(\"content\"):\n",
    "        current_analysis = f\"\"\"\n",
    "---\n",
    "CURRENT ANALYSIS ON FILE:\n",
    "- User ID: {state['user_id']}\n",
    "- Score: {state['score']}\n",
    "- Summary: {state['content']}\n",
    "- Total Guess: {state['total_guess']}\n",
    "---\n",
    "Use this analysis to inform your conversation. You can update it using the 'update_diagnosis' tool.\n",
    "\"\"\"\n",
    "    else:\n",
    "        current_analysis = (\n",
    "            \"No analysis on file yet. \"\n",
    "            \"Your goal is to talk to the user and gather information. \"\n",
    "            \"When ready, use the 'update_diagnosis' tool.\"\n",
    "        )\n",
    "        \n",
    "    return f\"{base_prompt}\\n{current_analysis}\"\n",
    "\n",
    "# 5. CREATE THE AGENT (Global Instance)\n",
    "\n",
    "# List of all tools\n",
    "tools = [retrieve_context, update_diagnosis]\n",
    "\n",
    "# Create the agent\n",
    "# This is a runnable and can be defined globally. It's stateless.\n",
    "# The *state* is handled by the checkpointer.\n",
    "agent = create_agent(\n",
    "    model=main_model,                 # Requirement #6\n",
    "    tools=tools,\n",
    "    state_schema=PsychologyAgentState, # Requirement #3 & #7\n",
    "    checkpointer=checkpointer,         # Requirement #1\n",
    "    # middleware=[\n",
    "    #     summarizer_middleware,       # Requirement #2\n",
    "    #     dynamic_psychology_prompt,   # Requirement #4 & #5\n",
    "    # ],\n",
    ")\n",
    "\n",
    "print(\"--- Psychology Chatbot Agent Created Successfully ---\")\n",
    "\n",
    "# # 6. CHAINLIT CHAT HANDLERS\n",
    "\n",
    "# @cl.on_chat_start\n",
    "# async def on_chat_start():\n",
    "#     \"\"\"\n",
    "#     Called when a new user session starts.\n",
    "#     \"\"\"\n",
    "#     # Create a unique thread_id for this Chainlit user session\n",
    "#     thread_id = str(uuid.uuid4())\n",
    "#     cl.user_session.set(\"thread_id\", thread_id)\n",
    "    \n",
    "#     # You would get the user_id from your authentication system.\n",
    "#     # For this example, we'll hardcode it.\n",
    "#     user_id = \"user_abc_123\"\n",
    "#     cl.user_session.set(\"user_id\", user_id)\n",
    "    \n",
    "#     # Send a welcome message\n",
    "#     await cl.Message(\n",
    "#         content=\"Hello! I'm your psychology assistant. \"\n",
    "#                 \"I'm here to listen. How are you feeling today?\\n\\n\"\n",
    "#                 \"(Note: I remember our conversations, but this is for reference only.)\"\n",
    "#     ).send()\n",
    "\n",
    "# @cl.on_message\n",
    "# async def on_message(message: cl.Message):\n",
    "#     \"\"\"\n",
    "#     Called every time the user sends a message.\n",
    "#     \"\"\"\n",
    "#     # 1. Get the user's unique thread_id from their session\n",
    "#     thread_id = cl.user_session.get(\"thread_id\")\n",
    "#     user_id = cl.user_session.get(\"user_id\")\n",
    "    \n",
    "#     # 2. Create the config for this specific thread\n",
    "#     config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "#     # 3. Check if this is the *first* message for this thread_id\n",
    "#     # We do this by checking if state exists in the database\n",
    "#     current_state = await checkpointer.aget(config)\n",
    "    \n",
    "#     # 4. Define the inputs for the agent\n",
    "#     inputs = {\"messages\": [(\"user\", message.content)]}\n",
    "    \n",
    "#     # If it's the first message (no state), we MUST include the\n",
    "#     # 'user_id' to initialize our custom PsychologyAgentState.\n",
    "#     if current_state is None:\n",
    "#         print(f\"First message for thread {thread_id}. Adding user_id.\")\n",
    "#         inputs[\"user_id\"] = user_id\n",
    "\n",
    "#     # 5. Stream the agent's response\n",
    "#     msg = cl.Message(content=\"\")\n",
    "#     await msg.send()\n",
    "    \n",
    "#     async for event in agent.astream(\n",
    "#         inputs,\n",
    "#         config,\n",
    "#         stream_mode=\"events\" # Use \"events\" to get token chunks\n",
    "#     ):\n",
    "#         kind = event[\"event\"]\n",
    "        \n",
    "#         # Stream the LLM's response tokens\n",
    "#         if kind == \"on_chat_model_stream\":\n",
    "#             chunk = event[\"data\"][\"chunk\"]\n",
    "#             if chunk.content:\n",
    "#                 await msg.stream_token(chunk.content)\n",
    "        \n",
    "#         # (Optional) Show tool calls\n",
    "#         elif kind == \"on_tool_start\":\n",
    "#             tool_name = event['data']['input']['tool']\n",
    "#             print(f\"Agent using tool: {tool_name}\")\n",
    "#             await msg.stream_token(f\"\\n*Thinking... (using: {tool_name})*\\n\")\n",
    "        \n",
    "#         elif kind == \"on_tool_end\":\n",
    "#             if event[\"data\"][\"output\"] and event[\"data\"][\"output\"].get(\"messages\"):\n",
    "#                 tool_output_msg = event[\"data\"][\"output\"][\"messages\"][0].content\n",
    "#                 print(f\"Tool output: {tool_output_msg}\")\n",
    "#                 # Don't show the user, just log it\n",
    "    \n",
    "#     # 6. Send the final, complete message\n",
    "#     await msg.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552ca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking Agent (Turn 1) ---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I've been feeling really down lately.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry to hear you're feeling down. Can you tell me more about what's been going on? What does \"down\" feel like for you?\n",
      "\n",
      "--- Invoking Agent (Turn 2) ---\n",
      "User: I'm also very stressed about my job.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm also very stressed about my job.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It sounds like you're dealing with a lot right now. Can you tell me more about the stress you're feeling regarding your job? What aspects of your job are causing you stress?\n",
      "\n",
      "--- Invoking Agent (Turn 3) ---\n",
      "User: I just feel no motivation.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I just feel no motivation.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I understand. It can be really tough to feel unmotivated. Have you noticed any changes in your sleep or appetite?\n",
      "\n",
      "I'm asking these questions to get a better understanding of how you're feeling, as it can help me provide more relevant information.\n"
     ]
    }
   ],
   "source": [
    "# A unique ID for each conversation thread\n",
    "thread_id = \"user-conversation-thread-1\"\n",
    "\n",
    "# A unique ID for the user\n",
    "user_id = \"user_abc_123\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "initial_state = {\n",
    "    \"messages\": [(\"user\", \"Hi, I've been feeling really down lately.\")],\n",
    "    \"user_id\": user_id,\n",
    "}\n",
    "\n",
    "print(\"\\n--- Invoking Agent (Turn 1) ---\")\n",
    "for event in agent.stream(initial_state, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Subsequent calls only need the new message.\n",
    "# The agent automatically loads the history and custom state from the DB.\n",
    "print(\"\\n--- Invoking Agent (Turn 2) ---\")\n",
    "print(\"User: I'm also very stressed about my job.\")\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"I'm also very stressed about my job.\")]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "print(\"\\n--- Invoking Agent (Turn 3) ---\")\n",
    "print(\"User: I just feel no motivation.\")\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"I just feel no motivation.\")]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-agent (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
